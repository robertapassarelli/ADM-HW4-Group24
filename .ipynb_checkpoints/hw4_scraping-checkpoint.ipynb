{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Group #24\n",
    "## 1) Does basic house information reflect house's description?\n",
    "\n",
    "Our goal is to implement two clustering and compare the results. We create two datasets and each of them will be filled by data that we scraped.\n",
    "\n",
    "First of all, we import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # For time sleep() Method\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests #requests.get\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import io\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "Web scraping using **Beautiful Soup** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : Tue Dec  4 13:49:24 2018\n",
      "['225.000', '2', '50', '1', '1']\n",
      "['189.000', '3', '79', '1', '1']\n",
      "['1.650.000', '5', '220', '3', '4']\n",
      "['1.050.000', '5', '290', '2', 'R']\n",
      "['1.250.000', '5', '185', '3', '0']\n",
      "['349.000', '3', '100', '1', '2']\n",
      "['1.350.000', '4', '200', '2', '2']\n",
      "['1.700.000', '5', '460', '3', '0']\n",
      "['690.000', '4', '127', '2', '1']\n",
      "['165.000', '3', '60', '1', 'A']\n",
      "End : Tue Dec  4 13:50:15 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ilaria T\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Start : %s\" % time.ctime())\n",
    "numb_ann = 10\n",
    "df1 = pd.DataFrame(columns=['Price', 'Locali', 'Superficie', 'Bagni', 'Piano'] )\n",
    "count = 1\n",
    "i = 1\n",
    "while count < numb_ann:\n",
    "    url = \"https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag=\"+str(i)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    mydivs = soup.find_all(\"div\", {\"class\": \"listing-item_body--content\"})\n",
    "    i +=1\n",
    "    for a in mydivs:\n",
    "        ann = \"announcement_\"+ str(count)\n",
    "        data = a.find_all(\"li\", {\"class\":\"lif__item\"})\n",
    "        #while len(data) == 6 and count <= numb_ann:\n",
    "        if len(data) == 6 and count <= numb_ann:\n",
    "            pr = price(data)\n",
    "            loc = locali(data)\n",
    "            mq = superficie(data)\n",
    "            bath = bagni(data) \n",
    "            floor = piano(data)\n",
    "            description = descrizione(soup)\n",
    "            l = [pr, loc, mq, bath, floor]\n",
    "            print(l)\n",
    "            df = pd.DataFrame([l], columns=['Prezzo', 'Locali', 'Superficie', 'Bagni', 'Piano'])\n",
    "            count +=1\n",
    "            df1 = pd.concat([df1, df], ignore_index=True) \n",
    "    # time.sleep(720)\n",
    "print (\"End : %s\" % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "print("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def descrizione(soup):\n",
    "    \n",
    "    stop_words = set(stopwords.words('italian')) #we take the stopwords we have to delete\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    all_links = soup.find_all('a')\n",
    "    links = []\n",
    "    for link in all_links:\n",
    "        if link.get('href')!= None:\n",
    "            links.append(link.get(\"href\"))\n",
    "            \n",
    "    need_links = []\n",
    "    for i in links:\n",
    "        if i[-4:] == 'html' and i[0:25] == 'https://www.immobiliare.it':\n",
    "            need_links.append(i)\n",
    "    \n",
    "    description_words = []\n",
    "    for link in need_links[:-1]:\n",
    "        site = urlopen(link)\n",
    "        page = BeautifulSoup(site, 'lxml')\n",
    "        description = page.find_all('div', class_=\"col-xs-12 description-text text-compressed\")\n",
    "        description = ''.join(str(description))\n",
    "        words = word_tokenize(description)\n",
    "        words_without_stop_words = [\"\" if word in stop_words and word.isalpha() else word for word in words]#we delete the stopwords\n",
    "        \n",
    "        new_words = \" \".join(words_without_stop_words)\n",
    "        b = tokenizer.tokenize(new_words)\n",
    "\n",
    "        l_stem_words = []\n",
    "        #with a for loop we stem all the words in the input, and store them in a list\n",
    "        for word in b:\n",
    "            l_stem_words.append(ps.stem(word))\n",
    "        #here we transform the list in a string\n",
    "        stem_words = ' '.join(l_stem_words).split()\n",
    "        description_words.append(stem_words)\n",
    "    return description_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary={}\n",
    "aux=0\n",
    "#loop to generate a vocabulary from all the documents with title, description and city\n",
    "vocabulary_text = vocabulary_creation(description_words, vocabulary)\n",
    "vocabulary = vocabulary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.tsv', 'w') as f: #vocabulary with all words is written in file 'vocabulary.tsv'\n",
    "    json.dump(vocabulary, f) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(d):\n",
    "    try:\n",
    "        price = d[0].contents[0].replace(\"\\n\", \"\").replace(\"€ \",\"\").strip()\n",
    "    except:\n",
    "        price = d[0].contents[0].get_text().replace(\"\\n\", \"\").replace(\"€ \",\"\").strip()\n",
    "\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locali(d):\n",
    "    local = d[1].get_text().replace(\"\\xa0\",\"\").replace(\"+\", \"\").replace(\"locali\", \"\").strip()\n",
    "    return local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superficie(d):\n",
    "    superf = d[2].get_text().replace(\"da \",\"\").replace(\"\\xa0m2\",\"\").replace(\"superficie\",\"\").strip()\n",
    "    return superf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagni(d):\n",
    "    bath = d[3].get_text().replace(\"\\xa0\",\"\").replace(\"+\", \"\").replace(\"bagni\", \"\").strip()\n",
    "    return bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piano(d):\n",
    "    floor = d[4].get_text().replace(\"\\xa0\", \"\").replace(\"\\n\",\"\").replace(\"T\",\"0\").replace(\"piano\",\"\").strip()\n",
    "    return floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary_creation(word_list,previous_vocabulary):\n",
    "    #create an new dictionary from a previous one which could be empty\n",
    "    vocabulary=previous_vocabulary\n",
    "    #obtain last index of previous dictionary and set first index for 1st different element in this vocabulary\n",
    "    if len(previous_vocabulary)==0:\n",
    "        number=1\n",
    "    else:\n",
    "        number=previous_vocabulary[list(previous_vocabulary)[len(previous_vocabulary)-1]]+1\n",
    "    #adding new words to the vocabulary and omitting repeated ones\n",
    "    for lista in word_list:\n",
    "        for word in lista:\n",
    "            index=list(vocabulary.keys())\n",
    "            if word in index:\n",
    "                pass\n",
    "            else:\n",
    "                vocabulary[word]=number\n",
    "                number+=1\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions defined in order to calculate the tfIdf coefficient\n",
    "def N(df1):\n",
    "    #number of documents in the collection\n",
    "    return len(df1)\n",
    "\n",
    "def df_t(word, df1):\n",
    "    #number of documents in the collection that contain a term t\n",
    "    counter=0\n",
    "    for lista in df1:\n",
    "        if word in lista:\n",
    "            counter+=1\n",
    "    return counter\n",
    "\n",
    "def tf_t_d(word,lista):\n",
    "    #term frequency:number of ocurrence of term t in document d\n",
    "    return lista.count(word)\n",
    "\n",
    "def id_f_t(word, df1):\n",
    "    #inverse document frequency of a term t\n",
    "    return log(len(df1)/df_t(word, df1),10)\n",
    "\n",
    "def tf_idf(word,lista,df1):\n",
    "    #tf_idf of a term in a document of a collection N\n",
    "    return tf_t_d(word,lista)*id_f_t(word, df1)\n",
    "\n",
    "def inverted_index_creation_tfIdf(vocabulary, df1):  \n",
    "    #function that creates inverted_index with coefficient tfIdf from all the documents\n",
    "    inverted_index={}\n",
    "    n=1\n",
    "    \n",
    "    for lista in df1:          \n",
    "         #loop for each word of the document create a new key if the word is not in the dictionary \n",
    "         #add the number of the document to an existing key is the word is in the dictionary\n",
    "         #tfIdf coefficient of each term is also added\n",
    "        for word in set(lista):\n",
    "    \n",
    "            index=list(vocabulary.values())\n",
    "            if vocabulary[word] in list(inverted_index.keys()):\n",
    "                inverted_index[vocabulary[word]]=(inverted_index[vocabulary[word]])+[(\"announcement_\"+str(n),tf_idf(word,lista,df1))]\n",
    "            else:\n",
    "                inverted_index[int(vocabulary[word])]=[(\"announcement_\"+str(n),tf_idf(word,lista,df1))]\n",
    "        \n",
    "        n+=1\n",
    "\n",
    "    with open('inverted_index_tfIdf.tsv', 'w') as f1: #write the inverted index in a file called \"inverted_index_tfIdf.tsv\"\n",
    "        json.dump(inverted_index, f1)\n",
    "        f1.close()\n",
    "    return inverted_index \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
