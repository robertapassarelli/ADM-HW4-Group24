{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Group #24\n",
    "## 1) Does basic house information reflect house's description?\n",
    "\n",
    "Our goal is to implement two clustering and compare the results. We create two datasets and each of them will be filled by data that we scraped.\n",
    "\n",
    "First of all, we import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # For time sleep() Method\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests #requests.get\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping & Datasets\n",
    "Web scraping using **Beautiful Soup** library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(d):\n",
    "    try:\n",
    "        price = d[0].contents[0].replace(\"\\n\", \"\").replace(\"€ \",\"\").strip()\n",
    "    except:\n",
    "        price = d[0].contents[0].get_text().replace(\"\\n\", \"\").replace(\"€ \",\"\").strip()\n",
    "\n",
    "    return price\n",
    "\n",
    "def locali(d):\n",
    "    local = d[1].get_text().replace(\"\\xa0\",\"\").replace(\"+\", \"\").replace(\"locali\", \"\").strip()\n",
    "    return local\n",
    "\n",
    "def superficie(d):\n",
    "    superf = d[2].get_text().replace(\"da \",\"\").replace(\"\\xa0m2\",\"\").replace(\"superficie\",\"\").strip()\n",
    "    return superf\n",
    "\n",
    "def bagni(d):\n",
    "    bath = d[3].get_text().replace(\"\\xa0\",\"\").replace(\"+\", \"\").replace(\"bagni\", \"\").strip()\n",
    "    return bath\n",
    "\n",
    "def piano(d):\n",
    "    floor = d[4].get_text().replace(\"\\xa0\", \"\").replace(\"\\n\",\"\").replace(\"T\",\"0\").replace(\"piano\",\"\").strip()\n",
    "    return floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def descrizione(need_links):\n",
    "    stop_words = set(stopwords.words('italian')) # we take the stopwords we have to delete\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    ps = PorterStemmer()\n",
    "    #description_words = []\n",
    "\n",
    "    for link in need_links[:-1]:\n",
    "        if link[0:5] == 'https':\n",
    "            site = urlopen(link)\n",
    "            page = BeautifulSoup(site, 'lxml')\n",
    "\n",
    "            description = page.find_all('div', class_=\"col-xs-12 description-text text-compressed\")\n",
    "            description = ''.join(str(description))\n",
    "\n",
    "            words = word_tokenize(description)\n",
    "            words_without_stop_words = [\"\" if word in stop_words else word for word in words] #we delete the stopwords\n",
    "            new_words = \" \".join(words_without_stop_words)\n",
    "            b = tokenizer.tokenize(new_words)\n",
    "\n",
    "            l_stem_words = []\n",
    "            # with a for loop we stem all the words in the input, and store them in a list\n",
    "            for word in b:\n",
    "                l_stem_words.append(ps.stem(word))\n",
    "            # here we transform the list in a string\n",
    "            stem_words = ' '.join(l_stem_words).split()\n",
    "            # description_words.append(stem_words)\n",
    "    #return description_words\n",
    "    return stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Start : %s\" % time.ctime()) # To see the time\n",
    "numb_ann = 10\n",
    "#df1 = pd.DataFrame(columns=['Price', 'Locali', 'Superficie', 'Bagni', 'Piano'] )\n",
    "count = 1\n",
    "i = 1 \n",
    "\n",
    "while count < numb_ann:\n",
    "    url = \"https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag=\"+str(i)\n",
    "    html = urlopen(url)\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    #mydivs = soup.find_all(\"div\", {\"class\": \"listing-item_body--content\"})\n",
    "    i += 1\n",
    "    \n",
    "    all_links = soup.find_all('a')\n",
    "    links = []\n",
    "\n",
    "    for link in all_links:\n",
    "        if link.get('href')!= None:\n",
    "            links.append(link.get(\"href\"))\n",
    "\n",
    "    need_links = []\n",
    "    for item in links:\n",
    "        if item[-4:] == 'html' and item[0:5] == 'https':\n",
    "            site = urlopen(item)\n",
    "            page = BeautifulSoup(site, 'lxml')\n",
    "            need_links.append(page)\n",
    "    print(need_links)        \n",
    "    \n",
    "    for a in need_links:\n",
    "        #ann = \"announcement_\"+ str(count)\n",
    "        #data = a.find_all(\"li\", {\"class\":\"lif__item\"})\n",
    "        data = a.find_all(\"div\", {\"class\": \"im-property__features\"})\n",
    "        print(len(data))\n",
    "        if len(data) == 6 and count <= numb_ann:\n",
    "            pr = price(data)\n",
    "            loc = locali(data)\n",
    "            mq = superficie(data)\n",
    "            bath = bagni(data) \n",
    "            floor = piano(data)\n",
    "            description = descrizione(need_links)\n",
    "            #print(description)\n",
    "            l = [pr, loc, mq, bath, floor, description]\n",
    "\n",
    "            df = pd.DataFrame([l], columns=['Price', 'Locali', 'Superficie', 'Bagni', 'Piano', 'Descrizione'])\n",
    "            count +=1\n",
    "            df1 = pd.concat([df1, df], ignore_index=True) \n",
    "            \n",
    "    # time.sleep(720)\n",
    "print (\"End : %s\" % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_links = soup.find_all('a')\n",
    "links = []\n",
    "\n",
    "for link in all_links:\n",
    "    if link.get('href')!= None:\n",
    "        links.append(link.get(\"href\"))\n",
    "\n",
    "need_links = []\n",
    "for i in links:\n",
    "    if i[-4:] == 'html':\n",
    "        need_links.append(i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary_creation(word_list, previous_vocabulary):\n",
    "    #create an new dictionary from a previous one which could be empty\n",
    "    vocabulary=previous_vocabulary\n",
    "    #obtain last index of previous dictionary and set first index for 1st different element in this vocabulary\n",
    "    if len(previous_vocabulary)==0:\n",
    "        number=1\n",
    "    else:\n",
    "        number=previous_vocabulary[list(previous_vocabulary)[len(previous_vocabulary)-1]]+1\n",
    "    #adding new words to the vocabulary and omitting repeated ones\n",
    "    for lista in word_list:\n",
    "        for word in lista:\n",
    "            index=list(vocabulary.keys())\n",
    "            if word in index:\n",
    "                pass\n",
    "            else:\n",
    "                vocabulary[word]=number\n",
    "                number+=1\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary={}\n",
    "aux=0\n",
    "# loop to generate a vocabulary from all the documents with title, description and city\n",
    "vocabulary_text = vocabulary_creation(description_words, vocabulary)\n",
    "vocabulary = vocabulary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vocabulary.tsv', 'w') as f: #vocabulary with all words is written in file 'vocabulary.tsv'\n",
    "    json.dump(vocabulary, f) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions defined in order to calculate the tfIdf coefficient\n",
    "def N(df1):\n",
    "    #number of documents in the collection\n",
    "    return len(df1)\n",
    "\n",
    "def df_t(word, df1):\n",
    "    #number of documents in the collection that contain a term t\n",
    "    counter=0\n",
    "    for lista in df1:\n",
    "        if word in lista:\n",
    "            counter+=1\n",
    "    return counter\n",
    "\n",
    "def tf_t_d(word,lista):\n",
    "    #term frequency:number of ocurrence of term t in document d\n",
    "    return lista.count(word)\n",
    "\n",
    "def id_f_t(word, df1):\n",
    "    #inverse document frequency of a term t\n",
    "    return log(len(df1)/df_t(word, df1),10)\n",
    "\n",
    "def tf_idf(word,lista,df1):\n",
    "    #tf_idf of a term in a document of a collection N\n",
    "    return tf_t_d(word,lista)*id_f_t(word, df1)\n",
    "\n",
    "def inverted_index_creation_tfIdf(vocabulary, df1):  \n",
    "    #function that creates inverted_index with coefficient tfIdf from all the documents\n",
    "    inverted_index={}\n",
    "    n=1\n",
    "    \n",
    "    for lista in df1:          \n",
    "         #loop for each word of the document create a new key if the word is not in the dictionary \n",
    "         #add the number of the document to an existing key is the word is in the dictionary\n",
    "         #tfIdf coefficient of each term is also added\n",
    "        for word in set(lista):\n",
    "    \n",
    "            index=list(vocabulary.values())\n",
    "            if vocabulary[word] in list(inverted_index.keys()):\n",
    "                inverted_index[vocabulary[word]]=(inverted_index[vocabulary[word]])+[(\"announcement_\"+str(n),tf_idf(word,lista,df1))]\n",
    "            else:\n",
    "                inverted_index[int(vocabulary[word])]=[(\"announcement_\"+str(n),tf_idf(word,lista,df1))]\n",
    "        \n",
    "        n+=1\n",
    "\n",
    "    with open('data/inverted_index_tfIdf.tsv', 'w') as f1: #write the inverted index in a file called \"inverted_index_tfIdf.tsv\"\n",
    "        json.dump(inverted_index, f1)\n",
    "        f1.close()\n",
    "    return inverted_index \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the **TF-IDF** of all the words, that is defined as the \"term frequency\" times the \"inverse document frequency\" where:\n",
    "- \"term frequency\" is the ratio between the number of the term occurencies in the document and the total number of words in the document;\n",
    "- \"inverse document frequency\" is the logarithm of the ratio between the total number of documents and the number of documents containing the term (plus 1 to avoid division by zero).\n",
    "\n",
    "IDF is indipendent from the specific document, thus we can calculate it once and use it when we need it. Each term will have a single IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted index \n",
    "inverted_index = {}  \n",
    "\n",
    "for key, value in .items():\n",
    "    for i in sorted(list(set(value))):\n",
    "        if (vocabulary[i] in inverted_index):\n",
    "            inverted_index[vocabulary[i]] = inverted_index[vocabulary[i]] + [key]\n",
    "        else:\n",
    "            inverted_index[vocabulary[i]] = [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with idf of all word\n",
    "idf = {}\n",
    "n_ann = len(df1)\n",
    "\n",
    "for term_id, doc in inverted_docd.items():\n",
    "    idf[term_id] = pd.np.log(n_ann/(1+len(doc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
