{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Group #24\n",
    "## Roberta Passarelli - Ilaria Taddei - Amirhossein Rajabi Shizari\n",
    "## 1) Does basic house information reflect house's description?\n",
    "\n",
    "Our goal is to implement two clustering and compare the results. We create two datasets and each of them will be filled by data that we scraped.\n",
    "\n",
    "First of all, we import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/robertapassarelli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time # For time.sleep() method\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import nltk # To remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import string # To remove punctuation\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "To create the dataset we have to scrape the website of [Immobiliare.it](https://www.immobiliare.it) using **Beautiful Soup** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : Sun Dec  9 10:23:04 2018\n"
     ]
    }
   ],
   "source": [
    "print (\"Start : %s\" % time.ctime())\n",
    "\n",
    "numb_pag = 5 # There are 25 announcements on each page\n",
    "df1 = pd.DataFrame(columns=['Price','Locali','Superficie','Bagni','Piano','Descrizione'] )\n",
    "i = 1\n",
    "\n",
    "while i <= numb_pag:\n",
    "    url = \"https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag=\"+str(i)\n",
    "    html = urlopen(url)\n",
    "    time.sleep(0.8) # to prevent the website block\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    all_links = soup.find_all('a')\n",
    "    \n",
    "    links = []\n",
    "    for link in all_links:\n",
    "        if link.get('href')!= None:\n",
    "            links.append(link.get(\"href\")) # list with all links\n",
    "\n",
    "    for item in links:\n",
    "        if item[-4:] == 'html' and item[0:5] == 'https':\n",
    "            site = urlopen(item)\n",
    "            soup = BeautifulSoup(site, 'lxml')\n",
    "            data = soup.find_all(\"ul\", {'class': \"list-inline list-piped features__list\"})\n",
    "            price = soup.find_all(\"ul\", {'class':\"list-inline features__price-block\"})\n",
    "            description = soup.find_all(\"div\",{'class':\"col-xs-12 description-text text-compressed\"})\n",
    "            try:\n",
    "                pr = int(price[0].contents[0].get_text().replace(\"â‚¬\",\"\").replace('.',''))\n",
    "                loc = data[0].contents[0].get_text().replace(\"\\xa0\",\"\").replace(\"+\", \"\").replace(\"locali\", \"\").strip()\n",
    "                mq = data[0].contents[1].get_text().replace(\"da \",\"\").replace(\"\\xa0m2\",\"\").replace(\"superficie\",\"\").replace(\"m2\",\"\").strip()\n",
    "                bath = data[0].contents[2].get_text().replace(\"\\xa0\",\"\").replace(\"+\", \"\").replace(\"bagni\", \"\").strip()\n",
    "                floor = data[0].contents[3].get_text().replace(\"\\xa0\", \"\").replace(\"\\n\",\"\").replace(\"piano\",\"\").replace(\"T\",\"0\").strip()\n",
    "                # we replace the floor `T` (piano Terra) with the number zero\n",
    "                descr = description[0].get_text()\n",
    "                l = [pr, loc, mq, bath, floor, descr]\n",
    "                df = pd.DataFrame([l], columns=['Price','Locali','Superficie','Bagni','Piano','Descrizione'])\n",
    "                df1 = pd.concat([df1, df], ignore_index = True) \n",
    "            except:\n",
    "                pass\n",
    "    i += 1\n",
    "print (\"End : %s\" % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean the data:\n",
    "- replace `\\n` character in the announcement's description with an empty space\n",
    "- drop all the announcements that don't have an integer for the `floor`\n",
    "- remove the point in the price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[df1['Piano'].apply(lambda x: str(x).isdigit() )]\n",
    "df1['Descrizione'] = df1['Descrizione'].str.replace(r'\\n', ' ', regex=True)\n",
    "df1['Price'] = df1['Price'].astype(str).str.strip()\n",
    "df1['Price'] = df1['Price'].str.replace('.', '')\n",
    "df2 = df1.values\n",
    "\n",
    "index = list(range(len(df1)))\n",
    "df1 = pd.DataFrame(df2, index = index, columns = ['Price','Locali','Superficie','Bagni','Piano','Descrizione'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the description in a `.txt` file and the dataframe in a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = list(df1.Descrizione)\n",
    "with open('data/description.txt', 'w') as file:\n",
    "     file.write(json.dumps(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('data/dataframe.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Information\n",
    "The first matrix `matrix1` is $m_{ij} = value$, where $i \\in \\{announcement_1, ..., announcement_n\\}$ ($n$ is the number of announcement) and $j \\in \\{price, locali, superficie, bagni, piano \\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = df1[['Price', 'Locali', 'Superficie', 'Bagni', 'Piano']]\n",
    "matrix1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Description\n",
    "The second matrix `descr_matrix` is $m_{ij} = \\text{tfIdf}_{ij}$ where $i \\in \\{announcement_1, ..., announcement_n\\}$ and $j \\in \\{word_1, ...,word_m\\}$, with $n$ number of the announcements and $m$ is the cardinality of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the `.txt` file with the descriptions\n",
    "with open('data/description.txt')as f: \n",
    "    description = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean the descriptions:\n",
    "- remove stopwords \n",
    "- remove punctuation\n",
    "- stemming\n",
    "\n",
    "Define a dictionary `clean_descr` with the structure:\n",
    "- key: number of announcement\n",
    "- value: cleaned word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('italian')) # set of stopwords\n",
    "stemmer = nltk.stem.snowball.ItalianStemmer() # italian stemmer\n",
    "punctuation = set(string.punctuation) # set of punctuation\n",
    "\n",
    "clean_descr = {}  \n",
    "\n",
    "for i, row in df1[:].iterrows():\n",
    "    # Turn the string in lowercase letter and remove the stopwords and non-alphabetic words\n",
    "    newrow = row.Descrizione\n",
    "    text = ([w.lower() for w in list(newrow.split(' ')) \n",
    "             if w.lower() not in stop_words and w.isalpha()])\n",
    "    # To remove punctuation considering character by character\n",
    "    l = []\n",
    "    for word in text:\n",
    "        word = ''.join(ch for ch in word if ch not in punctuation)\n",
    "        l.append(word)\n",
    "    # Stem the words and put in a dict\n",
    "    clean_descr[i] = [stemmer.stem(w) for w in l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create`vocabulary` like a dictionary of the words contained in all the documents that maps each word to an integer, with the structure:\n",
    "- key: word\n",
    "- value: number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {} # vocabulary as a dictionary\n",
    "i = 0\n",
    "\n",
    "for idx in range(len(clean_descr)):\n",
    "    for word in list(clean_descr.values())[idx]:\n",
    "        if word not in vocabulary.keys():\n",
    "            vocabulary[word] = i\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vocabulary.txt', 'w') as file:\n",
    "     file.write(json.dumps(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily proceed with the `inverted index`, a dictionary with:\n",
    "- key: number that correspond to the value in the vocabulary\n",
    "- value: number of announcement in which there is the word which corresponds to the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted index \n",
    "inverted_d = {} # empty dict \n",
    "\n",
    "for key, value in clean_descr.items():\n",
    "    for i in sorted(list(set(value))):\n",
    "        if (vocabulary[i] in inverted_d):\n",
    "            inverted_d[vocabulary[i]] = inverted_d[vocabulary[i]] + [key]\n",
    "        else:\n",
    "            inverted_d[vocabulary[i]] = [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/inverted_d.txt', 'w') as file:\n",
    "     file.write(json.dumps(inverted_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the **TF-IDF** of all the words, that is defined as the \"term frequency\" times the \"inverse document frequency\" where:\n",
    "- \"term frequency\" is the ratio between the number of the term occurencies in the announcement and the total number of words in the announcement;\n",
    "- \"inverse document frequency\" is the logarithm of the ratio between the total number of announcements and the number of announcements containing the term (plus 1 to avoid division by zero).\n",
    "\n",
    "IDF is indipendent from the specific announcement, thus we can calculate it once and use it when we need it. Each term will have a single IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = {}\n",
    "n_ann = len(clean_descr)\n",
    "\n",
    "for term_id, ann in inverted_d.items():\n",
    "    idf[term_id] = pd.np.log(n_ann/(1+len(ann)))\n",
    "    \n",
    "# inverted index \n",
    "tfidf_inverted_a = {} # empty dict \n",
    "\n",
    "for key, value in clean_descr.items():\n",
    "    for i in sorted(list(set(value))):\n",
    "        if (vocabulary[i] in tfidf_inverted_a):\n",
    "            tfidf_inverted_a[vocabulary[i]] = tfidf_inverted_a[vocabulary[i]] + [(key, (value.count(i)/len(value)) * idf[vocabulary[i]])]\n",
    "        else:\n",
    "            tfidf_inverted_a[vocabulary[i]] = [(key, (value.count(i)/len(value)) * idf[vocabulary[i]])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tfidf_inverted_a.txt', 'w') as file:\n",
    "     file.write(json.dumps(tfidf_inverted_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = []\n",
    "\n",
    "for word in tfidf_inverted_a.keys():\n",
    "    rows = []\n",
    "    indexis = list(map(lambda x:x[0],tfidf_inverted_a[word]))\n",
    "    for j in range(len(df1)):\n",
    "        if j in indexis:\n",
    "            idx = indexis.index(j)\n",
    "            rows.append(tfidf_inverted_a[word][idx][1])\n",
    "        else:\n",
    "            rows.append(0)\n",
    "    matrix.append(rows)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(tfidf_inverted_a.values())\n",
    "words = list(vocabulary.keys())\n",
    "\n",
    "words_ordered = []\n",
    "for i in words:\n",
    "    words_ordered.append(i)\n",
    "\n",
    "rows = set()\n",
    "for i in range(len(l)):\n",
    "    for j in range(len(l[i])):\n",
    "        rows.add(l[i][j][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the description matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_matrix = pd.DataFrame(matrix, index = words_ordered, columns = rows)\n",
    "descr_matrix = descr_matrix.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "descr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "This step consists in clustering the house announcements using *K-means++*. To choose the optimal number of clusters, we use the **Elbow-Method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try with the information matrix, `matrix1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(matrix1)\n",
    "    kmeanModel.fit(matrix1)\n",
    "    distortions.append(sum(np.min(cdist(matrix1, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / matrix1.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our scatterplot we can see that the optimal $k$ is $3$, because from that point on, if we add a $k$, it does not cause an evident difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans1 = KMeans(n_clusters=3)\n",
    "\n",
    "# Fitting the input data\n",
    "kmeans1 = kmeans1.fit(matrix1)\n",
    "labels1 = kmeans1.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = set() # empty set \n",
    "B = set()\n",
    "C = set()\n",
    "for i in range(len(matrix1)):\n",
    "    if labels1[i] == 0:\n",
    "        A.add(i)\n",
    "    elif labels1[i] == 1:\n",
    "        B.add(i)\n",
    "    elif labels1[i] == 2:\n",
    "        C.add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we see with the description matrix, `descr_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "distortions = []\n",
    "K = range(1,20)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(descr_matrix)\n",
    "    kmeanModel.fit(descr_matrix)\n",
    "    distortions.append(sum(np.min(cdist(descr_matrix, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / descr_matrix.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans2 = KMeans(n_clusters=3)\n",
    "# Fitting the input data\n",
    "kmeans2 = kmeans2.fit(descr_matrix)\n",
    "labels2 = kmeans2.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = set()\n",
    "B1 = set()\n",
    "C1 = set()\n",
    "for i in range(len(descr_matrix)):\n",
    "    if labels2[i] == 0:\n",
    "        A1.add(i)\n",
    "    elif labels2[i] == 1:\n",
    "        B1.add(i)\n",
    "    elif labels2[i] == 2:\n",
    "        C1.add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison among cluster\n",
    "\n",
    "#### Find similar clusters\n",
    "To check this, use the **Jaccard-Similarity** to measure the similarity betweeen the two outputs (information clusters vs description clusters). The Jaccard-Similarity is a statistic coefficient used for comparing the similarity and diversity of sample set. It is defined as the size of the intersection divided by the size of the union of the sample sets: $J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}.$\n",
    "\n",
    "Our goal is return the 3-most similar couples of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(S1, S2):\n",
    "    js = len(S1.intersection(S2))/len(S1.union(S2))\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_1 = [A, B, C]\n",
    "set_2 = [A1, B1, C1]\n",
    "n_set_1 = ['A', 'B', 'C']\n",
    "n_set_2 = ['A1', 'B1', 'C1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js = []\n",
    "jsd = {}\n",
    "jsd_name = {}\n",
    "for i in range(len(set_1)):\n",
    "    for j in range(len(set_2)):\n",
    "        c = jaccard_similarity(set_1[i], set_2[j])\n",
    "        js.append(c)\n",
    "        jsd[c] = [set_1[i], set_2[j]]\n",
    "        jsd_name[c] = [n_set_1[i], n_set_2[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(3):\n",
    "    m = max(js)\n",
    "    l.append(m)\n",
    "    js.remove(m)\n",
    "\n",
    "for i in l:\n",
    "    print(jsd_name[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the $3$ most similar couples of clusters betweeen the two outputs: information clusters and description clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word cloud of house descriptions\n",
    "We create a *wordcloud* for each couple of clusters. The words that will be represented are those extracted from the description of the houses that are in the relative couple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new dataframe `df_js` to save the Jacard Similarity, the name of the couple of clusters and the number of the considered announcements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_js = pd.DataFrame(columns=['Jac_similarity','Clusters','Announcements'] )\n",
    "for i in range(len(set_1)):\n",
    "    for j in range(len(set_2)):\n",
    "        jac_similarity = jaccard_similarity(set_1[i], set_2[j])\n",
    "        lab = [n_set_1[i], n_set_2[j]]\n",
    "        announc = set_1[i].union(set_2[j])\n",
    "        lst = [jac_similarity, lab, announc]   \n",
    "\n",
    "        df = pd.DataFrame([lst], columns=['Jac_similarity','Clusters','Announcements'])\n",
    "        df_js = pd.concat([df_js, df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from PIL import Image\n",
    "from os import getcwd, path\n",
    "stopwords = set(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(text, stopwords): \n",
    "    d = getcwd()\n",
    "    \n",
    "    mask = np.array(Image.open(path.join(d, \"data/cloud.png\")))\n",
    "    wc = WordCloud(background_color=\"white\", mask=mask, \n",
    "                   stopwords=stopwords, relative_scaling = 1.0, max_font_size=90)\n",
    "    wc.generate(text)\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the description of announcements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "txt = []\n",
    "for i in range(len(df_js)):\n",
    "    for j in df_js.iloc[i].Announcements:\n",
    "        text = text + df1.iloc[j].Descrizione\n",
    "    txt.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = txt[0]\n",
    "create_wordcloud(text, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = txt[1]\n",
    "create_wordcloud(text, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = txt[2]\n",
    "create_wordcloud(text, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
